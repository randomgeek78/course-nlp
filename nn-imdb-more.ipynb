{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling & Sentiment Analysis of IMDB movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=48\n",
    "# bs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      11.11% [1/9 00:04<00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-30:\n",
      "Process Process-22:\n",
      "Process Process-28:\n",
      "Process Process-32:\n",
      "Process Process-17:\n",
      "Process Process-18:\n",
      "Process Process-26:\n",
      "Process Process-27:\n",
      "Process Process-19:\n",
      "Process Process-20:\n",
      "Process Process-23:\n",
      "Process Process-25:\n",
      "Process Process-29:\n",
      "Traceback (most recent call last):\n",
      "Process Process-31:\n",
      "Traceback (most recent call last):\n",
      "Process Process-24:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process Process-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/concurrent/futures/process.py\", line 169, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/text/transform.py\u001b[0m in \u001b[0;36mprocess_all\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_all_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_by_cores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-62de81a7908a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data_lm = (TextList.from_folder(path)\n\u001b[1;32m      2\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mfilter_by_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unsup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0msplit_by_rand_pct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mlabel_for_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             .databunch(bs=bs, num_workers=1))\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_item_lists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelLists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_inner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;34m\"Process the inner datasets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m#progress_bar clear the outputs so in some case warnings issued during processing disappear.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, xp, yp, name, max_warn_items)\u001b[0m\n\u001b[1;32m    716\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mfilt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mfilt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, processor)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/text/data.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, ds)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/text/transform.py\u001b[0m in \u001b[0;36mprocess_all\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cpus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_all_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_all_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_by_cores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/concurrent/futures/process.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_management_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# objects that use file descriptors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_lm = (TextList.from_folder(path)\n",
    "            .filter_by_folder(include=['train', 'test', 'unsup']) \n",
    "            .split_by_rand_pct(0.1, seed=42)\n",
    "            .label_for_lm()           \n",
    "            .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "len(data_lm.vocab.itos),len(data_lm.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.label_for_lm??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.label_const??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.label_from_func??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm._label_from_list??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm._label_list??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save('lm_databunch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'lm_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=1.).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.436533</td>\n",
       "      <td>4.087692</td>\n",
       "      <td>0.287201</td>\n",
       "      <td>26:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.fit_one_cycle(1, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.00% [1/10 30:06<4:30:56]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.081352</td>\n",
       "      <td>3.883096</td>\n",
       "      <td>0.309451</td>\n",
       "      <td>30:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1976' class='' max='5374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      36.77% [1976/5374 10:37<18:16 4.0271]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a43843584ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearn_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_lm.unfreeze()\n",
    "learn_lm.fit_one_cycle(10, lr/10, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save('fine_tuned_10')\n",
    "learn_lm.save_encoder('fine_tuned_enc_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "             .split_by_folder(valid='test')\n",
    "             .label_from_folder(classes=['neg', 'pos'])\n",
    "             .databunch(bs=bs, num_workers=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.save('imdb_textlist_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, 'imdb_textlist_class', bs=bs, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5).to_fp16()\n",
    "learn_c.load_encoder('fine_tuned_enc_10')\n",
    "learn_c.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-2\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.292160</td>\n",
       "      <td>0.243463</td>\n",
       "      <td>0.899880</td>\n",
       "      <td>02:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.fit_one_cycle(1, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.save('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.241684</td>\n",
       "      <td>0.185892</td>\n",
       "      <td>0.928720</td>\n",
       "      <td>02:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.freeze_to(-2)\n",
    "learn_c.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.save('2nd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos i was bored , around 10 pm , so i watched this movie . xxmaj and i could not stop laughing . xxmaj everything was so ridiculous . xxmaj the way the kids were acting like they were older than 11 just cracked me up . xxmaj one of the kids had a ring , that supposedly killed people after 3 or so years . xxmaj it gave me the impression that he wanted to be a gangster . \n",
       " \n",
       "  xxmaj it 's pretty hard to take little kids seriously , especially when it has to do with eating worms . xxmaj they act like everything is such a big deal , like if xxmaj billy ( the main character ) does n't eat the worms then the world will end . \n",
       " \n",
       "  xxmaj this is a good movie for little kids ( excluding the fact that a 5 year old says ' penis ' ) , but not for teens or adults who do n't want to waste their time .,xxbos xxmaj bill ( xxmaj buddy xxmaj rogers ) is sent to xxmaj new xxmaj york by his uncle ( xxmaj richard xxmaj tucker ) to experience life before he inherits $ xxunk . xxmaj his uncle has paid 3 women xxmaj xxunk ( xxmaj kathryn xxmaj crawford ) , xxmaj maxine ( xxmaj josephine xxmaj dunn ) and xxmaj pauline ( xxmaj carole xxmaj lombard ) to chaperone him and ensure that he does not fall foul of gold - diggers . xxmaj one such lady xxmaj cleo ( xxmaj geneva xxmaj mitchell ) turns up on the scene to the xxunk of the women . xxmaj we follow the tale as the girls are offered more money to appear in a show instead of their escorting role that they have agreed to carry out for the 3 months that xxmaj bill is in xxmaj new xxmaj york , while xxmaj bill meets with xxmaj cleo and another woman . xxmaj at the end , love is in the air for xxmaj bill and one other xxrep 13 . \n",
       " \n",
       "  xxmaj the picture quality and sound quality are poor in this film . xxmaj the story is interspersed with musical numbers but the songs are bad and xxmaj kathryn xxmaj crawford has a terrible voice . xxmaj rogers is n't that good either . xxmaj he 's pleasant enough but only really comes to life when playing the drums or trombone . xxmaj there is a very irritating character who plays a cab driver ( xxmaj roscoe xxmaj karns ) and the film is just dull .,xxbos ( xxunk one truly memorable part of this otherwise rather dull and tepid bit of xxmaj british cuisine is xxmaj steiner 's henna rinse , one of the worst dye jobs ever . xxmaj that , and the magnificent caterpillar eyebrows on the old evil dude who was trying to steal xxmaj steiner 's invention . xxup mst3 k does an admirable job of making a wretchedly boring and grey film funny . i particularly like it when xxmaj crow kills xxmaj mike with his ' touch of death ' , and when he revives him in the theatre , xxmaj mike cries \" xxmaj guys , i died , i saw eternal truth and beauty ! oh , it 's this movie ... \" xxmaj that would be a letdown , having to come back from the afterlife to watch the rest of xxmaj the xxmaj projected xxmaj man . xxmaj the film could make a fortune being sold as a sleep aide . xxmaj some of the puns in the film were wicked : police xxunk ! \" xxunk , is n't it ? \" police xxunk 's lowe , all right \" xxmaj tom xxunk low , right down by the floor ! \" police xxunk i get on ? \" xxmaj tom xxunk 's dead , but knock yourself out \" xxup mst3 k is definitely the only way to watch this snoozer .,xxbos i have seen it . xxmaj it 's not \" good \" but interesting in an understated way . xxmaj the boys in it are quite naturalistic but xxrep 16 . the graphic / gratuitous final gang rape scene is repugnant and -oh xxunk the arbitrary insertion of second world war footage is offensive in the way it attempts to compare real horror with this misogynistic contrivance . xxmaj real atrocity is xxunk this film is just atrocious . xxmaj however , the film has a look which can draw you in . xxmaj but it seems to me that is the \" xxmaj emperor 's xxmaj new xxmaj clothes \" , but in fact in reverse . xxmaj the film looks good , but the direction , story , content and final feeling you take away from this film is vacuous . xxmaj if a feeling can be vacuous - this is it .,xxbos xxmaj and i repeat , please do not see this movie ! xxmaj this is more than a review . xxmaj this is a warning . xxmaj this sets the record for the worst , most effortless comedy ever made . xxmaj at least with most of the recent comedies nowadays , the gags are crude and flat , but the writers and directors put in at least some sort of effort into making them funny . i never get tired of repeating one of my favorite xxunk : xxmaj everyone thinks they can do comedy , and only 10 percent of them are right . xxmaj comedy is hard ! xxmaj this is not some genre any fool can play around with . i think it 's atrocious that the filmmakers are comparing this piece of garbage to \" xxmaj kentucky xxmaj fried xxmaj movie . \" xxmaj basically , these bozos are comparing their so - called comic talents to those of the brilliant xxmaj jim xxmaj abrahams and the xxmaj zucker xxmaj brothers . xxmaj come on , i 've seen xxmaj pauly xxmaj shore movies that are 10 times funnier than \" xxmaj the xxmaj underground xxmaj comedy xxmaj movie . \" xxmaj here 's a sample of the comedy for those curious about seeing this movie : xxmaj one sketch involves a superhero dressed like a penis named xxmaj xxunk . xxmaj the whole joke is that he defeats his enemies by squirting them with semen . xxmaj that 's it . xxmaj that 's the whole joke . xxmaj wow . xxmaj this is enough to make xxmaj carrot xxmaj top roll his eyes . xxmaj another sketch involves a man having sex with a dead person in a porn movie . xxmaj and in another sketch , there 's a bag lady beauty contest , in which we 're exposed to the horrible sights of bikini - clad middle - aged women with beer guts and stretch marks . xxmaj plus , making fun of the homeless is more sad than funny . xxmaj it 's a step away from mocking the mentally handicapped . xxmaj the whole movie is supposed to be a satire . i think the filmmakers forgot that a key element of satire ... is xxup truth ! ! ! xxmaj for anybody who actually enjoyed this crap , explain to me what is truthful about xxup any of these gags ! xxmaj some of the sketches might 've sounded funny on paper , but anybody who 's taken any screen writing classes knows that if a sight gag sounds too funny on paper , it probably wo n't be funny on screen . xxmaj if i tell someone about a big , black , muscular gay virgin , who 's saving himself for the right man , he or she would probably laugh . xxmaj but watching the premise played out on screen for about 10 minutes is a complete drag . i hate how whenever people criticize a low - brow comedy like this for not being funny , they 're regarded as stuck - up squares . i just saw \" xxmaj white xxmaj chicks \" recently . xxmaj that 's another low - brow , politically incorrect comedy , but i laughed my head off . xxmaj the most offensive thing about \" xxmaj the xxmaj underground xxmaj comedy xxmaj movie \" is it 's not funny ! xxmaj what the writers and directors do n't understand is that merely being filthy and tasteless does n't work . xxmaj there has to be more ! xxmaj just think of the famous scene from \" xxmaj there 's xxmaj something xxmaj about xxmaj mary \" ( ironically , enough the bozo filmmakers put the xxmaj farrellys on their special thanks list ) . xxmaj the joke about the semen was n't just funny because it involved bodily fluids . xxmaj there was a buildup . xxmaj ben xxmaj stiller was masturbating in the bathroom to make sure he did n't go out on a date with a \" loaded gun . \" xxmaj then he looked around to see where all the semen went after it was released . a knock is on the door , and he has to answer it . xxmaj his date , xxmaj mary , is at the door and that 's when it 's revealed that the semen is hanging off xxmaj ben 's ear . xxmaj in this movie , there are multiple gags involving characters squirting loads of semen at people , with no buildup whatsoever . xxmaj as xxmaj jay xxmaj leno always says , \" xxmaj this comedy thing 's not so easy , is it ? \" xxmaj keep that in mind , xxmaj vince xxmaj offer , 'cause you were n't cut out for this genre ! ! xxmaj the only reason people might laugh at these gags is because they want to feel hip . xxmaj let 's face it , nowadays it 's hip to laugh at anything politically incorrect . i know comedy is subjective ... but this movie should n't be funny to anybody , except maybe the filmmakers themselves . xxmaj as a side note , the movie had to have been made before xxmaj michael xxmaj clarke xxmaj duncan 's fame in movies like \" xxmaj armageddon \" and \" xxmaj the xxmaj green xxmaj mile . \" xxmaj there ca n't be any other reason why an actor of his caliber would volunteer to be part of this amateurish freak show . xxmaj all the others in the cast are either non - actors , has - been actors or b - movie stars . xxmaj karen xxmaj black made a good impression in \" xxmaj five xxmaj easy xxmaj pieces , \" but i do n't think she 's done anything of value ever since . xxmaj slash was probably drugged into being in this film . xxmaj gina xxmaj lee xxmaj xxunk is nothing without \" xxmaj baywatch . \" xxmaj xxunk is the film 's biggest star ( keeping in mind xxmaj duncan was n't famous at the time ) , and there are still probably a ton of people who have n't heard of her -- for good reason . xxmaj usually , i 'm in support of extremely low - budget flicks , but this one deserves to drift into obscurity . i hope to xxmaj lord this does n't become a cult classic ! xxmaj should n't there be a law against distributing crap like this ?\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos i did n't know what to expect when i started watching this movie , by the end of it i was pulling my hairs out . xxmaj this was one of the most pathetic movies of this year ... in fact , in the last ten years . xxmaj david xxmaj dhawan should just give up his career as a director . i am yet to come across one original script that xxmaj david xxmaj dhawan has worked on . xxmaj this one was a complete bit y bit rip off xxmaj hitch . i have nothing against remakes as such , but this one is just so lousy that it makes you even hate the original one ( which was pretty decent ) . i fail to understand what actors like xxmaj salman and xxmaj govinda saw in this script . i read somewhere , that this was supposed to be xxmaj govinda 's comeback vehicle . xxmaj if that s true , then only xxmaj god can save his career . xxmaj salman just overacted to the hilt . xxmaj govinda who i think is an actor of very high caliber was completely wasted . xxmaj katrina xxmaj kaif and lara xxmaj dutta had nothing to do apart form wearing designer clothes and smiling for no rhyme or reason . xxmaj please stay away form this one !,xxbos xxmaj for several reasons , this movie is simply awful . xxmaj other posters have listed some of this movie 's historical errors . xxmaj well , i have a layman 's knowledge of xxmaj roman history and even i found the inaccuracies flagrant . i usually forgive errors in historical movies because i understand that the purpose is to entertain not educate . xxmaj and shrinking a long saga down to a two hour feature requires some , let 's say , historical license . xxmaj but this movie goes well beyond mere rounding . \n",
       " \n",
       "  xxmaj there 's worse . xxmaj to tell a story from a distant period , the movie uses flashbacks which just make the story more confusing . xxmaj unless viewers have some prior knowledge of the period , they will quickly be lost . xxmaj in addition , the movie was obviously filmed simultaneously in xxmaj italian and xxmaj english with various actors being dubbed later . xxmaj at times , the actors seem as if they were in completely different movies which were then edited together . xxmaj in fact , this is not far wrong . xxmaj the actors were obviously pasted onto a cheesy computer generated ancient xxmaj rome . \n",
       " \n",
       "  xxmaj the only reason i give this boring mess any stars is because i always find xxmaj peter o'toole entertaining . xxmaj but that is no reason to rent it . xxmaj if you are curious about xxmaj roman history , there are much better movies available .,xxbos xxmaj remember those terrible war movies your grandmother forced you to watch 25 or so years ago on your old xxup vhs recorder ? \" xxmaj the xxmaj fallen \" is just a bad executed remake of those movies ! xxmaj the story is terrible , the direction is terrible , the editing is terrible , the music is terrible , and all together make an unbearable nightmare . \n",
       " \n",
       "  xxmaj it is also terribly slow ! xxmaj very slow ! i tried to sleep while watching it but i could n't do it because i had nightmares of it . \n",
       " \n",
       "  xxmaj please do n't watch this movie ! xxmaj it is xxup that bad ! xxmaj ten lines is a lot so i do n't know what else to say . \n",
       " \n",
       "  xxmaj press the eject button xxup now and you wo nt regret it !,xxbos a post - apocalyptic warrior goes off to save some kind of xxmaj nun and on the way meets some cyber - punks on skates who want to kick his ass . xxmaj this is one of the hardest to watch films ever , xxmaj there are scenes with silence that seems to last hours before somebody comes out with the next badly written , badly acted line . xxmaj there are action sequences that keep repeating - and we 're not talking the quickfire 1 - 2 - 3 action repeat on a particularly good kick that was made popular by eastern directors , we 're talking many , many repeats of long , bad fight sequences . xxmaj this is incredibly confusing at first but then quickly becomes annoying as you 're watching a 30 second sequence for the 2nd , 3rd and 4th time . xxmaj any kind of plot or vision is lost within the confusing continuity , the only thing that s keeps this film in the xxunk ( apart from the bet from a friend that i could n't watch it all the way through without begging for it to be turned off and disposed off safely so it may harm no - one else ) is the fact that although painful , this film is unintentionally hilarious , i 'm not at all a fan of those \" so bad that it 's funny \" type of films but at parts i was in tears . xxmaj other points to note are the quality of the sound and picture but this is forgiveable as it 's obvious money was a major problem in the making of this film . xxmaj final verdict - xxmaj king of the \" so bad they 're funny \" genre , anybody having that kind of genre video night should get themselves a copy . xxmaj also lets not forget that it is actually the worst film i 've ever seen .,xxbos i believe that this movie was a terrible waste of my time , and i would know after watching it 5 times in class . this movie does not show what absolutely perfectly happened during these times . no one can truly say that these things happened to the letter . if anything the only good part would be the actors , even tho that they were really really xxunk were reading the script without expression . quite boring . i would rather watch play school . so i would definitely like to never ever see this movie again in my whole life . it is a complete waste of time unless you want your time to be wasted and if you would like to see an unrealistic view of what happened back in 1981 .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f08a2f6aae8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos i was bored , around 10 pm , so i watched this movie . xxmaj and i could not stop laughing . xxmaj everything was so ridiculous . xxmaj the way the kids were acting like they were older than 11 just cracked me up . xxmaj one of the kids had a ring , that supposedly killed people after 3 or so years . xxmaj it gave me the impression that he wanted to be a gangster . \n",
       " \n",
       "  xxmaj it 's pretty hard to take little kids seriously , especially when it has to do with eating worms . xxmaj they act like everything is such a big deal , like if xxmaj billy ( the main character ) does n't eat the worms then the world will end . \n",
       " \n",
       "  xxmaj this is a good movie for little kids ( excluding the fact that a 5 year old says ' penis ' ) , but not for teens or adults who do n't want to waste their time .,xxbos xxmaj bill ( xxmaj buddy xxmaj rogers ) is sent to xxmaj new xxmaj york by his uncle ( xxmaj richard xxmaj tucker ) to experience life before he inherits $ xxunk . xxmaj his uncle has paid 3 women xxmaj xxunk ( xxmaj kathryn xxmaj crawford ) , xxmaj maxine ( xxmaj josephine xxmaj dunn ) and xxmaj pauline ( xxmaj carole xxmaj lombard ) to chaperone him and ensure that he does not fall foul of gold - diggers . xxmaj one such lady xxmaj cleo ( xxmaj geneva xxmaj mitchell ) turns up on the scene to the xxunk of the women . xxmaj we follow the tale as the girls are offered more money to appear in a show instead of their escorting role that they have agreed to carry out for the 3 months that xxmaj bill is in xxmaj new xxmaj york , while xxmaj bill meets with xxmaj cleo and another woman . xxmaj at the end , love is in the air for xxmaj bill and one other xxrep 13 . \n",
       " \n",
       "  xxmaj the picture quality and sound quality are poor in this film . xxmaj the story is interspersed with musical numbers but the songs are bad and xxmaj kathryn xxmaj crawford has a terrible voice . xxmaj rogers is n't that good either . xxmaj he 's pleasant enough but only really comes to life when playing the drums or trombone . xxmaj there is a very irritating character who plays a cab driver ( xxmaj roscoe xxmaj karns ) and the film is just dull .,xxbos ( xxunk one truly memorable part of this otherwise rather dull and tepid bit of xxmaj british cuisine is xxmaj steiner 's henna rinse , one of the worst dye jobs ever . xxmaj that , and the magnificent caterpillar eyebrows on the old evil dude who was trying to steal xxmaj steiner 's invention . xxup mst3 k does an admirable job of making a wretchedly boring and grey film funny . i particularly like it when xxmaj crow kills xxmaj mike with his ' touch of death ' , and when he revives him in the theatre , xxmaj mike cries \" xxmaj guys , i died , i saw eternal truth and beauty ! oh , it 's this movie ... \" xxmaj that would be a letdown , having to come back from the afterlife to watch the rest of xxmaj the xxmaj projected xxmaj man . xxmaj the film could make a fortune being sold as a sleep aide . xxmaj some of the puns in the film were wicked : police xxunk ! \" xxunk , is n't it ? \" police xxunk 's lowe , all right \" xxmaj tom xxunk low , right down by the floor ! \" police xxunk i get on ? \" xxmaj tom xxunk 's dead , but knock yourself out \" xxup mst3 k is definitely the only way to watch this snoozer .,xxbos i have seen it . xxmaj it 's not \" good \" but interesting in an understated way . xxmaj the boys in it are quite naturalistic but xxrep 16 . the graphic / gratuitous final gang rape scene is repugnant and -oh xxunk the arbitrary insertion of second world war footage is offensive in the way it attempts to compare real horror with this misogynistic contrivance . xxmaj real atrocity is xxunk this film is just atrocious . xxmaj however , the film has a look which can draw you in . xxmaj but it seems to me that is the \" xxmaj emperor 's xxmaj new xxmaj clothes \" , but in fact in reverse . xxmaj the film looks good , but the direction , story , content and final feeling you take away from this film is vacuous . xxmaj if a feeling can be vacuous - this is it .,xxbos xxmaj and i repeat , please do not see this movie ! xxmaj this is more than a review . xxmaj this is a warning . xxmaj this sets the record for the worst , most effortless comedy ever made . xxmaj at least with most of the recent comedies nowadays , the gags are crude and flat , but the writers and directors put in at least some sort of effort into making them funny . i never get tired of repeating one of my favorite xxunk : xxmaj everyone thinks they can do comedy , and only 10 percent of them are right . xxmaj comedy is hard ! xxmaj this is not some genre any fool can play around with . i think it 's atrocious that the filmmakers are comparing this piece of garbage to \" xxmaj kentucky xxmaj fried xxmaj movie . \" xxmaj basically , these bozos are comparing their so - called comic talents to those of the brilliant xxmaj jim xxmaj abrahams and the xxmaj zucker xxmaj brothers . xxmaj come on , i 've seen xxmaj pauly xxmaj shore movies that are 10 times funnier than \" xxmaj the xxmaj underground xxmaj comedy xxmaj movie . \" xxmaj here 's a sample of the comedy for those curious about seeing this movie : xxmaj one sketch involves a superhero dressed like a penis named xxmaj xxunk . xxmaj the whole joke is that he defeats his enemies by squirting them with semen . xxmaj that 's it . xxmaj that 's the whole joke . xxmaj wow . xxmaj this is enough to make xxmaj carrot xxmaj top roll his eyes . xxmaj another sketch involves a man having sex with a dead person in a porn movie . xxmaj and in another sketch , there 's a bag lady beauty contest , in which we 're exposed to the horrible sights of bikini - clad middle - aged women with beer guts and stretch marks . xxmaj plus , making fun of the homeless is more sad than funny . xxmaj it 's a step away from mocking the mentally handicapped . xxmaj the whole movie is supposed to be a satire . i think the filmmakers forgot that a key element of satire ... is xxup truth ! ! ! xxmaj for anybody who actually enjoyed this crap , explain to me what is truthful about xxup any of these gags ! xxmaj some of the sketches might 've sounded funny on paper , but anybody who 's taken any screen writing classes knows that if a sight gag sounds too funny on paper , it probably wo n't be funny on screen . xxmaj if i tell someone about a big , black , muscular gay virgin , who 's saving himself for the right man , he or she would probably laugh . xxmaj but watching the premise played out on screen for about 10 minutes is a complete drag . i hate how whenever people criticize a low - brow comedy like this for not being funny , they 're regarded as stuck - up squares . i just saw \" xxmaj white xxmaj chicks \" recently . xxmaj that 's another low - brow , politically incorrect comedy , but i laughed my head off . xxmaj the most offensive thing about \" xxmaj the xxmaj underground xxmaj comedy xxmaj movie \" is it 's not funny ! xxmaj what the writers and directors do n't understand is that merely being filthy and tasteless does n't work . xxmaj there has to be more ! xxmaj just think of the famous scene from \" xxmaj there 's xxmaj something xxmaj about xxmaj mary \" ( ironically , enough the bozo filmmakers put the xxmaj farrellys on their special thanks list ) . xxmaj the joke about the semen was n't just funny because it involved bodily fluids . xxmaj there was a buildup . xxmaj ben xxmaj stiller was masturbating in the bathroom to make sure he did n't go out on a date with a \" loaded gun . \" xxmaj then he looked around to see where all the semen went after it was released . a knock is on the door , and he has to answer it . xxmaj his date , xxmaj mary , is at the door and that 's when it 's revealed that the semen is hanging off xxmaj ben 's ear . xxmaj in this movie , there are multiple gags involving characters squirting loads of semen at people , with no buildup whatsoever . xxmaj as xxmaj jay xxmaj leno always says , \" xxmaj this comedy thing 's not so easy , is it ? \" xxmaj keep that in mind , xxmaj vince xxmaj offer , 'cause you were n't cut out for this genre ! ! xxmaj the only reason people might laugh at these gags is because they want to feel hip . xxmaj let 's face it , nowadays it 's hip to laugh at anything politically incorrect . i know comedy is subjective ... but this movie should n't be funny to anybody , except maybe the filmmakers themselves . xxmaj as a side note , the movie had to have been made before xxmaj michael xxmaj clarke xxmaj duncan 's fame in movies like \" xxmaj armageddon \" and \" xxmaj the xxmaj green xxmaj mile . \" xxmaj there ca n't be any other reason why an actor of his caliber would volunteer to be part of this amateurish freak show . xxmaj all the others in the cast are either non - actors , has - been actors or b - movie stars . xxmaj karen xxmaj black made a good impression in \" xxmaj five xxmaj easy xxmaj pieces , \" but i do n't think she 's done anything of value ever since . xxmaj slash was probably drugged into being in this film . xxmaj gina xxmaj lee xxmaj xxunk is nothing without \" xxmaj baywatch . \" xxmaj xxunk is the film 's biggest star ( keeping in mind xxmaj duncan was n't famous at the time ) , and there are still probably a ton of people who have n't heard of her -- for good reason . xxmaj usually , i 'm in support of extremely low - budget flicks , but this one deserves to drift into obscurity . i hope to xxmaj lord this does n't become a cult classic ! xxmaj should n't there be a law against distributing crap like this ?\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos i did n't know what to expect when i started watching this movie , by the end of it i was pulling my hairs out . xxmaj this was one of the most pathetic movies of this year ... in fact , in the last ten years . xxmaj david xxmaj dhawan should just give up his career as a director . i am yet to come across one original script that xxmaj david xxmaj dhawan has worked on . xxmaj this one was a complete bit y bit rip off xxmaj hitch . i have nothing against remakes as such , but this one is just so lousy that it makes you even hate the original one ( which was pretty decent ) . i fail to understand what actors like xxmaj salman and xxmaj govinda saw in this script . i read somewhere , that this was supposed to be xxmaj govinda 's comeback vehicle . xxmaj if that s true , then only xxmaj god can save his career . xxmaj salman just overacted to the hilt . xxmaj govinda who i think is an actor of very high caliber was completely wasted . xxmaj katrina xxmaj kaif and lara xxmaj dutta had nothing to do apart form wearing designer clothes and smiling for no rhyme or reason . xxmaj please stay away form this one !,xxbos xxmaj for several reasons , this movie is simply awful . xxmaj other posters have listed some of this movie 's historical errors . xxmaj well , i have a layman 's knowledge of xxmaj roman history and even i found the inaccuracies flagrant . i usually forgive errors in historical movies because i understand that the purpose is to entertain not educate . xxmaj and shrinking a long saga down to a two hour feature requires some , let 's say , historical license . xxmaj but this movie goes well beyond mere rounding . \n",
       " \n",
       "  xxmaj there 's worse . xxmaj to tell a story from a distant period , the movie uses flashbacks which just make the story more confusing . xxmaj unless viewers have some prior knowledge of the period , they will quickly be lost . xxmaj in addition , the movie was obviously filmed simultaneously in xxmaj italian and xxmaj english with various actors being dubbed later . xxmaj at times , the actors seem as if they were in completely different movies which were then edited together . xxmaj in fact , this is not far wrong . xxmaj the actors were obviously pasted onto a cheesy computer generated ancient xxmaj rome . \n",
       " \n",
       "  xxmaj the only reason i give this boring mess any stars is because i always find xxmaj peter o'toole entertaining . xxmaj but that is no reason to rent it . xxmaj if you are curious about xxmaj roman history , there are much better movies available .,xxbos xxmaj remember those terrible war movies your grandmother forced you to watch 25 or so years ago on your old xxup vhs recorder ? \" xxmaj the xxmaj fallen \" is just a bad executed remake of those movies ! xxmaj the story is terrible , the direction is terrible , the editing is terrible , the music is terrible , and all together make an unbearable nightmare . \n",
       " \n",
       "  xxmaj it is also terribly slow ! xxmaj very slow ! i tried to sleep while watching it but i could n't do it because i had nightmares of it . \n",
       " \n",
       "  xxmaj please do n't watch this movie ! xxmaj it is xxup that bad ! xxmaj ten lines is a lot so i do n't know what else to say . \n",
       " \n",
       "  xxmaj press the eject button xxup now and you wo nt regret it !,xxbos a post - apocalyptic warrior goes off to save some kind of xxmaj nun and on the way meets some cyber - punks on skates who want to kick his ass . xxmaj this is one of the hardest to watch films ever , xxmaj there are scenes with silence that seems to last hours before somebody comes out with the next badly written , badly acted line . xxmaj there are action sequences that keep repeating - and we 're not talking the quickfire 1 - 2 - 3 action repeat on a particularly good kick that was made popular by eastern directors , we 're talking many , many repeats of long , bad fight sequences . xxmaj this is incredibly confusing at first but then quickly becomes annoying as you 're watching a 30 second sequence for the 2nd , 3rd and 4th time . xxmaj any kind of plot or vision is lost within the confusing continuity , the only thing that s keeps this film in the xxunk ( apart from the bet from a friend that i could n't watch it all the way through without begging for it to be turned off and disposed off safely so it may harm no - one else ) is the fact that although painful , this film is unintentionally hilarious , i 'm not at all a fan of those \" so bad that it 's funny \" type of films but at parts i was in tears . xxmaj other points to note are the quality of the sound and picture but this is forgiveable as it 's obvious money was a major problem in the making of this film . xxmaj final verdict - xxmaj king of the \" so bad they 're funny \" genre , anybody having that kind of genre video night should get themselves a copy . xxmaj also lets not forget that it is actually the worst film i 've ever seen .,xxbos i believe that this movie was a terrible waste of my time , and i would know after watching it 5 times in class . this movie does not show what absolutely perfectly happened during these times . no one can truly say that these things happened to the letter . if anything the only good part would be the actors , even tho that they were really really xxunk were reading the script without expression . quite boring . i would rather watch play school . so i would definitely like to never ever see this movie again in my whole life . it is a complete waste of time unless you want your time to be wasted and if you would like to see an unrealistic view of what happened back in 1981 .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f08a2f6aae8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0, MixedPrecision\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos i was bored , around 10 pm , so i watched this movie . xxmaj and i could not stop laughing . xxmaj everything was so ridiculous . xxmaj the way the kids were acting like they were older than 11 just cracked me up . xxmaj one of the kids had a ring , that supposedly killed people after 3 or so years . xxmaj it gave me the impression that he wanted to be a gangster . \n",
       " \n",
       "  xxmaj it 's pretty hard to take little kids seriously , especially when it has to do with eating worms . xxmaj they act like everything is such a big deal , like if xxmaj billy ( the main character ) does n't eat the worms then the world will end . \n",
       " \n",
       "  xxmaj this is a good movie for little kids ( excluding the fact that a 5 year old says ' penis ' ) , but not for teens or adults who do n't want to waste their time .,xxbos xxmaj bill ( xxmaj buddy xxmaj rogers ) is sent to xxmaj new xxmaj york by his uncle ( xxmaj richard xxmaj tucker ) to experience life before he inherits $ xxunk . xxmaj his uncle has paid 3 women xxmaj xxunk ( xxmaj kathryn xxmaj crawford ) , xxmaj maxine ( xxmaj josephine xxmaj dunn ) and xxmaj pauline ( xxmaj carole xxmaj lombard ) to chaperone him and ensure that he does not fall foul of gold - diggers . xxmaj one such lady xxmaj cleo ( xxmaj geneva xxmaj mitchell ) turns up on the scene to the xxunk of the women . xxmaj we follow the tale as the girls are offered more money to appear in a show instead of their escorting role that they have agreed to carry out for the 3 months that xxmaj bill is in xxmaj new xxmaj york , while xxmaj bill meets with xxmaj cleo and another woman . xxmaj at the end , love is in the air for xxmaj bill and one other xxrep 13 . \n",
       " \n",
       "  xxmaj the picture quality and sound quality are poor in this film . xxmaj the story is interspersed with musical numbers but the songs are bad and xxmaj kathryn xxmaj crawford has a terrible voice . xxmaj rogers is n't that good either . xxmaj he 's pleasant enough but only really comes to life when playing the drums or trombone . xxmaj there is a very irritating character who plays a cab driver ( xxmaj roscoe xxmaj karns ) and the film is just dull .,xxbos ( xxunk one truly memorable part of this otherwise rather dull and tepid bit of xxmaj british cuisine is xxmaj steiner 's henna rinse , one of the worst dye jobs ever . xxmaj that , and the magnificent caterpillar eyebrows on the old evil dude who was trying to steal xxmaj steiner 's invention . xxup mst3 k does an admirable job of making a wretchedly boring and grey film funny . i particularly like it when xxmaj crow kills xxmaj mike with his ' touch of death ' , and when he revives him in the theatre , xxmaj mike cries \" xxmaj guys , i died , i saw eternal truth and beauty ! oh , it 's this movie ... \" xxmaj that would be a letdown , having to come back from the afterlife to watch the rest of xxmaj the xxmaj projected xxmaj man . xxmaj the film could make a fortune being sold as a sleep aide . xxmaj some of the puns in the film were wicked : police xxunk ! \" xxunk , is n't it ? \" police xxunk 's lowe , all right \" xxmaj tom xxunk low , right down by the floor ! \" police xxunk i get on ? \" xxmaj tom xxunk 's dead , but knock yourself out \" xxup mst3 k is definitely the only way to watch this snoozer .,xxbos i have seen it . xxmaj it 's not \" good \" but interesting in an understated way . xxmaj the boys in it are quite naturalistic but xxrep 16 . the graphic / gratuitous final gang rape scene is repugnant and -oh xxunk the arbitrary insertion of second world war footage is offensive in the way it attempts to compare real horror with this misogynistic contrivance . xxmaj real atrocity is xxunk this film is just atrocious . xxmaj however , the film has a look which can draw you in . xxmaj but it seems to me that is the \" xxmaj emperor 's xxmaj new xxmaj clothes \" , but in fact in reverse . xxmaj the film looks good , but the direction , story , content and final feeling you take away from this film is vacuous . xxmaj if a feeling can be vacuous - this is it .,xxbos xxmaj and i repeat , please do not see this movie ! xxmaj this is more than a review . xxmaj this is a warning . xxmaj this sets the record for the worst , most effortless comedy ever made . xxmaj at least with most of the recent comedies nowadays , the gags are crude and flat , but the writers and directors put in at least some sort of effort into making them funny . i never get tired of repeating one of my favorite xxunk : xxmaj everyone thinks they can do comedy , and only 10 percent of them are right . xxmaj comedy is hard ! xxmaj this is not some genre any fool can play around with . i think it 's atrocious that the filmmakers are comparing this piece of garbage to \" xxmaj kentucky xxmaj fried xxmaj movie . \" xxmaj basically , these bozos are comparing their so - called comic talents to those of the brilliant xxmaj jim xxmaj abrahams and the xxmaj zucker xxmaj brothers . xxmaj come on , i 've seen xxmaj pauly xxmaj shore movies that are 10 times funnier than \" xxmaj the xxmaj underground xxmaj comedy xxmaj movie . \" xxmaj here 's a sample of the comedy for those curious about seeing this movie : xxmaj one sketch involves a superhero dressed like a penis named xxmaj xxunk . xxmaj the whole joke is that he defeats his enemies by squirting them with semen . xxmaj that 's it . xxmaj that 's the whole joke . xxmaj wow . xxmaj this is enough to make xxmaj carrot xxmaj top roll his eyes . xxmaj another sketch involves a man having sex with a dead person in a porn movie . xxmaj and in another sketch , there 's a bag lady beauty contest , in which we 're exposed to the horrible sights of bikini - clad middle - aged women with beer guts and stretch marks . xxmaj plus , making fun of the homeless is more sad than funny . xxmaj it 's a step away from mocking the mentally handicapped . xxmaj the whole movie is supposed to be a satire . i think the filmmakers forgot that a key element of satire ... is xxup truth ! ! ! xxmaj for anybody who actually enjoyed this crap , explain to me what is truthful about xxup any of these gags ! xxmaj some of the sketches might 've sounded funny on paper , but anybody who 's taken any screen writing classes knows that if a sight gag sounds too funny on paper , it probably wo n't be funny on screen . xxmaj if i tell someone about a big , black , muscular gay virgin , who 's saving himself for the right man , he or she would probably laugh . xxmaj but watching the premise played out on screen for about 10 minutes is a complete drag . i hate how whenever people criticize a low - brow comedy like this for not being funny , they 're regarded as stuck - up squares . i just saw \" xxmaj white xxmaj chicks \" recently . xxmaj that 's another low - brow , politically incorrect comedy , but i laughed my head off . xxmaj the most offensive thing about \" xxmaj the xxmaj underground xxmaj comedy xxmaj movie \" is it 's not funny ! xxmaj what the writers and directors do n't understand is that merely being filthy and tasteless does n't work . xxmaj there has to be more ! xxmaj just think of the famous scene from \" xxmaj there 's xxmaj something xxmaj about xxmaj mary \" ( ironically , enough the bozo filmmakers put the xxmaj farrellys on their special thanks list ) . xxmaj the joke about the semen was n't just funny because it involved bodily fluids . xxmaj there was a buildup . xxmaj ben xxmaj stiller was masturbating in the bathroom to make sure he did n't go out on a date with a \" loaded gun . \" xxmaj then he looked around to see where all the semen went after it was released . a knock is on the door , and he has to answer it . xxmaj his date , xxmaj mary , is at the door and that 's when it 's revealed that the semen is hanging off xxmaj ben 's ear . xxmaj in this movie , there are multiple gags involving characters squirting loads of semen at people , with no buildup whatsoever . xxmaj as xxmaj jay xxmaj leno always says , \" xxmaj this comedy thing 's not so easy , is it ? \" xxmaj keep that in mind , xxmaj vince xxmaj offer , 'cause you were n't cut out for this genre ! ! xxmaj the only reason people might laugh at these gags is because they want to feel hip . xxmaj let 's face it , nowadays it 's hip to laugh at anything politically incorrect . i know comedy is subjective ... but this movie should n't be funny to anybody , except maybe the filmmakers themselves . xxmaj as a side note , the movie had to have been made before xxmaj michael xxmaj clarke xxmaj duncan 's fame in movies like \" xxmaj armageddon \" and \" xxmaj the xxmaj green xxmaj mile . \" xxmaj there ca n't be any other reason why an actor of his caliber would volunteer to be part of this amateurish freak show . xxmaj all the others in the cast are either non - actors , has - been actors or b - movie stars . xxmaj karen xxmaj black made a good impression in \" xxmaj five xxmaj easy xxmaj pieces , \" but i do n't think she 's done anything of value ever since . xxmaj slash was probably drugged into being in this film . xxmaj gina xxmaj lee xxmaj xxunk is nothing without \" xxmaj baywatch . \" xxmaj xxunk is the film 's biggest star ( keeping in mind xxmaj duncan was n't famous at the time ) , and there are still probably a ton of people who have n't heard of her -- for good reason . xxmaj usually , i 'm in support of extremely low - budget flicks , but this one deserves to drift into obscurity . i hope to xxmaj lord this does n't become a cult classic ! xxmaj should n't there be a law against distributing crap like this ?\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos i did n't know what to expect when i started watching this movie , by the end of it i was pulling my hairs out . xxmaj this was one of the most pathetic movies of this year ... in fact , in the last ten years . xxmaj david xxmaj dhawan should just give up his career as a director . i am yet to come across one original script that xxmaj david xxmaj dhawan has worked on . xxmaj this one was a complete bit y bit rip off xxmaj hitch . i have nothing against remakes as such , but this one is just so lousy that it makes you even hate the original one ( which was pretty decent ) . i fail to understand what actors like xxmaj salman and xxmaj govinda saw in this script . i read somewhere , that this was supposed to be xxmaj govinda 's comeback vehicle . xxmaj if that s true , then only xxmaj god can save his career . xxmaj salman just overacted to the hilt . xxmaj govinda who i think is an actor of very high caliber was completely wasted . xxmaj katrina xxmaj kaif and lara xxmaj dutta had nothing to do apart form wearing designer clothes and smiling for no rhyme or reason . xxmaj please stay away form this one !,xxbos xxmaj for several reasons , this movie is simply awful . xxmaj other posters have listed some of this movie 's historical errors . xxmaj well , i have a layman 's knowledge of xxmaj roman history and even i found the inaccuracies flagrant . i usually forgive errors in historical movies because i understand that the purpose is to entertain not educate . xxmaj and shrinking a long saga down to a two hour feature requires some , let 's say , historical license . xxmaj but this movie goes well beyond mere rounding . \n",
       " \n",
       "  xxmaj there 's worse . xxmaj to tell a story from a distant period , the movie uses flashbacks which just make the story more confusing . xxmaj unless viewers have some prior knowledge of the period , they will quickly be lost . xxmaj in addition , the movie was obviously filmed simultaneously in xxmaj italian and xxmaj english with various actors being dubbed later . xxmaj at times , the actors seem as if they were in completely different movies which were then edited together . xxmaj in fact , this is not far wrong . xxmaj the actors were obviously pasted onto a cheesy computer generated ancient xxmaj rome . \n",
       " \n",
       "  xxmaj the only reason i give this boring mess any stars is because i always find xxmaj peter o'toole entertaining . xxmaj but that is no reason to rent it . xxmaj if you are curious about xxmaj roman history , there are much better movies available .,xxbos xxmaj remember those terrible war movies your grandmother forced you to watch 25 or so years ago on your old xxup vhs recorder ? \" xxmaj the xxmaj fallen \" is just a bad executed remake of those movies ! xxmaj the story is terrible , the direction is terrible , the editing is terrible , the music is terrible , and all together make an unbearable nightmare . \n",
       " \n",
       "  xxmaj it is also terribly slow ! xxmaj very slow ! i tried to sleep while watching it but i could n't do it because i had nightmares of it . \n",
       " \n",
       "  xxmaj please do n't watch this movie ! xxmaj it is xxup that bad ! xxmaj ten lines is a lot so i do n't know what else to say . \n",
       " \n",
       "  xxmaj press the eject button xxup now and you wo nt regret it !,xxbos a post - apocalyptic warrior goes off to save some kind of xxmaj nun and on the way meets some cyber - punks on skates who want to kick his ass . xxmaj this is one of the hardest to watch films ever , xxmaj there are scenes with silence that seems to last hours before somebody comes out with the next badly written , badly acted line . xxmaj there are action sequences that keep repeating - and we 're not talking the quickfire 1 - 2 - 3 action repeat on a particularly good kick that was made popular by eastern directors , we 're talking many , many repeats of long , bad fight sequences . xxmaj this is incredibly confusing at first but then quickly becomes annoying as you 're watching a 30 second sequence for the 2nd , 3rd and 4th time . xxmaj any kind of plot or vision is lost within the confusing continuity , the only thing that s keeps this film in the xxunk ( apart from the bet from a friend that i could n't watch it all the way through without begging for it to be turned off and disposed off safely so it may harm no - one else ) is the fact that although painful , this film is unintentionally hilarious , i 'm not at all a fan of those \" so bad that it 's funny \" type of films but at parts i was in tears . xxmaj other points to note are the quality of the sound and picture but this is forgiveable as it 's obvious money was a major problem in the making of this film . xxmaj final verdict - xxmaj king of the \" so bad they 're funny \" genre , anybody having that kind of genre video night should get themselves a copy . xxmaj also lets not forget that it is actually the worst film i 've ever seen .,xxbos i believe that this movie was a terrible waste of my time , and i would know after watching it 5 times in class . this movie does not show what absolutely perfectly happened during these times . no one can truly say that these things happened to the letter . if anything the only good part would be the actors , even tho that they were really really xxunk were reading the script without expression . quite boring . i would rather watch play school . so i would definitely like to never ever see this movie again in my whole life . it is a complete waste of time unless you want your time to be wasted and if you would like to see an unrealistic view of what happened back in 1981 .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f08a2f6aae8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)\n",
       "loss_scale: 65536\n",
       "max_noskip: 1000\n",
       "dynamic: True\n",
       "clip: None\n",
       "flat_master: False\n",
       "max_scale: 16777216\n",
       "loss_fp32: True], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_c.load('2nd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.198448</td>\n",
       "      <td>0.165060</td>\n",
       "      <td>0.937800</td>\n",
       "      <td>04:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.freeze_to(-3)\n",
    "learn_c.fit_one_cycle(1, slice(lr/2/(2.6**4),lr/2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.save('3rd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.169140</td>\n",
       "      <td>0.157897</td>\n",
       "      <td>0.939440</td>\n",
       "      <td>04:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125248</td>\n",
       "      <td>0.159314</td>\n",
       "      <td>0.942160</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.unfreeze()\n",
    "learn_c.fit_one_cycle(2, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.120063</td>\n",
       "      <td>0.145701</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>03:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.087303</td>\n",
       "      <td>0.152943</td>\n",
       "      <td>0.948080</td>\n",
       "      <td>03:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.unfreeze()\n",
    "learn_c.fit_one_cycle(2, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.save('clas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__ 57\n",
      "__doc__ 113\n",
      "__package__ 16\n",
      "__loader__ 16\n",
      "__spec__ 16\n",
      "__builtin__ 80\n",
      "__builtins__ 80\n",
      "_ih 264\n",
      "_oh 240\n",
      "_dh 72\n",
      "In 264\n",
      "Out 240\n",
      "get_ipython 64\n",
      "exit 56\n",
      "quit 56\n",
      "_ 56\n",
      "__ 49\n",
      "___ 49\n",
      "_i 54\n",
      "_ii 55\n",
      "_iii 53\n",
      "_i1 152\n",
      "version 80\n",
      "AUROC 1056\n",
      "AccumulateScheduler 1056\n",
      "AdamW 80\n",
      "AdaptiveConcatPool2d 1056\n",
      "AffineFunc 888\n",
      "AffineMatrix 1056\n",
      "AnnealFunc 888\n",
      "Any 48\n",
      "AnyStr 88\n",
      "ArgStar 888\n",
      "AverageMetric 1056\n",
      "BCEFlat 136\n",
      "BCEWithLogitsFlat 136\n",
      "BatchNorm1dFlat 1056\n",
      "BatchSampler 1056\n",
      "BatchSamples 888\n",
      "BnFreeze 1056\n",
      "BoolOrTensor 80\n",
      "BufferedWriter 400\n",
      "ByteTensor 400\n",
      "BytesIO 400\n",
      "Callable 888\n",
      "Callback 1056\n",
      "CallbackHandler 1056\n",
      "CallbackList 888\n",
      "Category 1056\n",
      "CategoryList 1056\n",
      "CategoryProcessor 1056\n",
      "ClassificationInterpretation 1056\n",
      "Collection 888\n",
      "Config 1056\n",
      "ConfusionMatrix 1056\n",
      "Counter 1056\n",
      "CrossEntropyFlat 136\n",
      "DataBunch 1056\n",
      "DataFrame 1056\n",
      "DataFrameOrChunks 80\n",
      "DataLoader 1464\n",
      "Dataset 1056\n",
      "DatasetType 1056\n",
      "Debugger 1056\n",
      "DeviceDataLoader 1056\n",
      "Dict 888\n",
      "DistributedDataParallel 1056\n",
      "DoubleTensor 400\n",
      "EmptyLabel 1056\n",
      "EmptyLabelList 1056\n",
      "Enum 1056\n",
      "ExpRMSPE 1056\n",
      "ExplainedVariance 1056\n",
      "F 80\n",
      "FBeta 1056\n",
      "FilePathList 888\n",
      "Flatten 1056\n",
      "FlattenedLoss 1056\n",
      "FloatItem 1056\n",
      "FloatList 1056\n",
      "FloatOrTensor 80\n",
      "FloatTensor 400\n",
      "Floats 80\n",
      "GradientClipping 1056\n",
      "HalfTensor 400\n",
      "Hashable 888\n",
      "HookFunc 888\n",
      "ImgLabel 400\n",
      "ImgLabels 888\n",
      "InitVar 1056\n",
      "IntEnum 1056\n",
      "IntOrTensor 80\n",
      "Interpretation 1056\n",
      "IntsOrStrs 80\n",
      "ItemBase 1056\n",
      "ItemList 1056\n",
      "ItemLists 1056\n",
      "ItemsList 888\n",
      "Iterable 888\n",
      "Iterator 888\n",
      "KWArgs 888\n",
      "KappaScore 1056\n",
      "KeyFunc 888\n",
      "LabelList 1184\n",
      "LabelLists 1056\n",
      "LabelSmoothingCrossEntropy 1056\n",
      "Lambda 1056\n",
      "LambdaFunc 888\n",
      "LayerFunc 888\n",
      "Learner 1056\n",
      "LearnerCallback 1056\n",
      "LightingFunc 888\n",
      "List 888\n",
      "ListOrItem 80\n",
      "ListRules 888\n",
      "ListSizes 888\n",
      "LogitTensorImage 1056\n",
      "LongTensor 400\n",
      "LossFunction 888\n",
      "MSELossFlat 136\n",
      "Mapping 888\n",
      "MasterBar 1056\n",
      "MatthewsCorreff 1056\n",
      "MergeLayer 1056\n",
      "MetricFunc 888\n",
      "MetricFuncList 888\n",
      "MetricsList 888\n",
      "MixedItem 1056\n",
      "MixedItemList 1056\n",
      "MixedProcessor 1056\n",
      "ModelOnCPU 1056\n",
      "Module 1056\n",
      "ModuleList 888\n",
      "MultiCategory 1056\n",
      "MultiCategoryList 1056\n",
      "MultiCategoryProcessor 1056\n",
      "MultiLabelClassificationInterpretation 1056\n",
      "MultiLabelFbeta 1056\n",
      "NPArray 400\n",
      "NPArrayList 888\n",
      "NPArrayMask 400\n",
      "NPArrayableList 888\n",
      "NPImage 400\n",
      "NewType 136\n",
      "NoneReduceOnCPU 1056\n",
      "NoopLoss 1056\n",
      "NormType 1056\n",
      "Number 888\n",
      "OptDataFrame 80\n",
      "OptListOrItem 80\n",
      "OptLossFunc 80\n",
      "OptMetrics 80\n",
      "OptOptimizer 80\n",
      "OptRange 80\n",
      "OptSplitFunc 80\n",
      "OptStats 80\n",
      "OptStrList 80\n",
      "OptStrTuple 80\n",
      "OptimWrapper 1184\n",
      "Optional 48\n",
      "OrderedDict 400\n",
      "PBar 1056\n",
      "ParamList 888\n",
      "ParameterModule 1184\n",
      "PartialLayer 1056\n",
      "Patch 1056\n",
      "Path 888\n",
      "PathLikeOrBinaryStream 80\n",
      "PathOrStr 80\n",
      "Perplexity 1056\n",
      "PixelFunc 888\n",
      "PixelShuffle_ICNR 1056\n",
      "Point 888\n",
      "Points 888\n",
      "PoolFlatten 136\n",
      "PooledSelfAttention2d 1056\n",
      "PrePostInitMeta 1056\n",
      "PreProcessor 1056\n",
      "Precision 1056\n",
      "PrettyString 1056\n",
      "ProcessPoolExecutor 1056\n",
      "ProgressBar 1056\n",
      "R2Score 1056\n",
      "RMSE 1056\n",
      "Rank0Tensor 136\n",
      "Recall 1056\n",
      "RecordOnCPU 1056\n",
      "Recorder 1464\n",
      "ResizeBatch 1056\n",
      "Sampler 1056\n",
      "Scheduler 1056\n",
      "SelfAttention 1056\n",
      "Sequence 888\n",
      "SequentialEx 1056\n",
      "Series 1056\n",
      "ShortTensor 400\n",
      "ShowGraph 1056\n",
      "SigmoidRange 1056\n",
      "SimpleNamespace 400\n",
      "Sized 888\n",
      "Sizes 888\n",
      "SmoothenValue 1056\n",
      "SplitArrayList 888\n",
      "SplitFunc 888\n",
      "SplitFuncOrIdxList 80\n",
      "StartOptEnd 80\n",
      "StrList 888\n",
      "Tensor 1056\n",
      "TensorDataset 1056\n",
      "TensorImage 1056\n",
      "TensorImageSize 888\n",
      "TensorOrNumList 888\n",
      "TensorOrNumber 80\n",
      "Tensors 80\n",
      "TfmList 80\n",
      "ThreadPoolExecutor 1056\n",
      "Tokens 888\n",
      "Tuple 888\n",
      "TypeVar 888\n",
      "URLs 1056\n",
      "Union 80\n",
      "View 1056\n",
      "WassersteinLoss 1056\n",
      "Weights 888\n",
      "abc 80\n",
      "abstractmethod 136\n",
      "abstractproperty 1056\n",
      "accuracy 136\n",
      "accuracy_thresh 136\n",
      "add_metrics 136\n",
      "annealing_cos 136\n",
      "annealing_exp 136\n",
      "annealing_linear 136\n",
      "annealing_no 136\n",
      "annealing_poly 136\n",
      "apply_init 136\n",
      "apply_leaf 136\n",
      "arange_of 136\n",
      "array 136\n",
      "arrays_split 136\n",
      "as_tensor 72\n",
      "attrgetter 400\n",
      "auc_roc_score 136\n",
      "batch_to_half 136\n",
      "batchnorm_2d 136\n",
      "bias_types 104\n",
      "bn2float 136\n",
      "bn_drop_lin 136\n",
      "bn_types 72\n",
      "bunzip 136\n",
      "bz2 80\n",
      "callbacks 80\n",
      "camel2snake 136\n",
      "children 136\n",
      "children_and_parameters 136\n",
      "chunks 136\n",
      "collections 80\n",
      "compose 136\n",
      "concurrent 80\n",
      "cond_init 136\n",
      "contextmanager 136\n",
      "conv2d 136\n",
      "conv2d_trans 136\n",
      "conv_layer 136\n",
      "copy 136\n",
      "cos 216\n",
      "csv 80\n",
      "data_collate 136\n",
      "dataclass 136\n",
      "datapath4file 136\n",
      "deepcopy 136\n",
      "defaultdict 400\n",
      "defaults 48\n",
      "df_names_to_idx 136\n",
      "dice 136\n",
      "distrib_barrier 136\n",
      "doc 136\n",
      "download_data 136\n",
      "download_url 136\n",
      "embedding 136\n",
      "error_rate 136\n",
      "even_mults 136\n",
      "exp 216\n",
      "exp_rmspe 136\n",
      "explained_variance 136\n",
      "extract_kwargs 136\n",
      "fastai_types 2280\n",
      "fbeta 136\n",
      "field 136\n",
      "find_classes 136\n",
      "first_el 136\n",
      "first_layer 136\n",
      "fit 136\n",
      "fit_fc 136\n",
      "fit_one_cycle 136\n",
      "flatten_check 136\n",
      "flatten_model 136\n",
      "float_or_x 136\n",
      "foreground_acc 136\n",
      "func_args 136\n",
      "functools 80\n",
      "gc 80\n",
      "get_files 136\n",
      "get_model 136\n",
      "get_param_groups 136\n",
      "get_preds 136\n",
      "get_tmp_file 136\n",
      "grab_idx 136\n",
      "gzip 80\n",
      "has_arg 136\n",
      "has_params 136\n",
      "hashlib 80\n",
      "have_min_pkg_version 136\n",
      "html 80\n",
      "icnr 136\n",
      "idx_dict 136\n",
      "ifnone 136\n",
      "importlib 80\n",
      "in_channels 136\n",
      "index_row 136\n",
      "init_default 136\n",
      "inspect 80\n",
      "io 80\n",
      "is1d 136\n",
      "is_dict 136\n",
      "is_listy 136\n",
      "is_pathlike 136\n",
      "is_pool_type 136\n",
      "is_tuple 136\n",
      "itemgetter 400\n",
      "itertools 80\n",
      "join_path 136\n",
      "join_paths 136\n",
      "json 80\n",
      "last_layer 136\n",
      "listify 136\n",
      "load_data 136\n",
      "load_learner 136\n",
      "loadtxt_str 136\n",
      "log 216\n",
      "log_uniform 136\n",
      "logit 136\n",
      "logit_ 136\n",
      "loss_batch 136\n",
      "lr_find 136\n",
      "mae 136\n",
      "master_bar 1464\n",
      "math 80\n",
      "mean_absolute_error 136\n",
      "mean_squared_error 136\n",
      "mean_squared_logarithmic_error 136\n",
      "mimetypes 80\n",
      "mixup 136\n",
      "model2half 136\n",
      "model_type 136\n",
      "mse 136\n",
      "msle 136\n",
      "namedtuple 136\n",
      "nn 80\n",
      "no_wd_types 80\n",
      "noop 136\n",
      "np 80\n",
      "np2model_tensor 136\n",
      "np_address 136\n",
      "np_func 136\n",
      "num_children 136\n",
      "num_cpus 136\n",
      "num_distrib 136\n",
      "numbers 80\n",
      "one_cycle_scheduler 136\n",
      "one_hot 136\n",
      "one_param 136\n",
      "operator 80\n",
      "optim 80\n",
      "os 80\n",
      "parallel 136\n",
      "partial 400\n",
      "partition 136\n",
      "partition_by_cores 136\n",
      "patches 80\n",
      "patheffects 80\n",
      "pathlib 80\n",
      "pd 80\n",
      "pd_max_colwidth 136\n",
      "pickle 80\n",
      "pkg_resources 80\n",
      "plt 80\n",
      "progress_bar 1464\n",
      "r2_score 136\n",
      "rand_bool 136\n",
      "random 80\n",
      "random_split 136\n",
      "range_children 136\n",
      "range_of 136\n",
      "rank_distrib 136\n",
      "re 80\n",
      "recurse 136\n",
      "recurse_eq 136\n",
      "reduce 72\n",
      "relu 136\n",
      "remove_module_load 136\n",
      "requests 80\n",
      "requires_grad 136\n",
      "res_block 136\n",
      "rmse 136\n",
      "roc_curve 136\n",
      "root_mean_squared_error 136\n",
      "save_texts 136\n",
      "scipy 80\n",
      "series2cat 136\n",
      "set_all_seed 136\n",
      "set_bn_eval 136\n",
      "set_trace 136\n",
      "show_some 136\n",
      "shutil 80\n",
      "sigmoid_range 136\n",
      "simple_cnn 136\n",
      "sin 216\n",
      "spectral_norm 136\n",
      "split_kwargs_by_func 136\n",
      "split_model 136\n",
      "split_model_idx 136\n",
      "split_no_wd_params 136\n",
      "subplots 136\n",
      "subprocess 80\n",
      "sys 80\n",
      "tan 216\n",
      "tanh 216\n",
      "tarfile 80\n",
      "tempfile 80\n",
      "tensor 136\n",
      "text2html_table 136\n",
      "to_cpu 136\n",
      "to_data 136\n",
      "to_detach 136\n",
      "to_device 136\n",
      "to_float 136\n",
      "to_fp16 136\n",
      "to_fp32 136\n",
      "to_half 136\n",
      "to_int 136\n",
      "to_np 136\n",
      "top_k_accuracy 136\n",
      "torch 80\n",
      "train_epoch 136\n",
      "trainable_params 136\n",
      "trange_of 136\n",
      "trunc_normal_ 136\n",
      "try_import 136\n",
      "try_int 136\n",
      "try_save 136\n",
      "typing 80\n",
      "uniform 136\n",
      "uniform_int 136\n",
      "uniqueify 136\n",
      "untar_data 136\n",
      "url2name 136\n",
      "url2path 136\n",
      "validate 136\n",
      "warn 72\n",
      "warnings 80\n",
      "weakref 80\n",
      "weight_norm 136\n",
      "working_directory 136\n",
      "yaml 80\n",
      "__version__ 55\n",
      "RNNLearner 2000\n",
      "LanguageLearner 1056\n",
      "convert_weights 136\n",
      "decode_spec_tokens 136\n",
      "get_language_model 136\n",
      "language_model_learner 136\n",
      "MultiBatchEncoder 1464\n",
      "get_text_classifier 136\n",
      "text_classifier_learner 136\n",
      "PoolingLinearClassifier 1184\n",
      "LanguageModelPreLoader 888\n",
      "SortSampler 1056\n",
      "SortishSampler 1056\n",
      "TextList 1056\n",
      "pad_collate 136\n",
      "TextDataBunch 1056\n",
      "TextLMDataBunch 1184\n",
      "TextClasDataBunch 1184\n",
      "Text 1056\n",
      "open_text 136\n",
      "TokenizeProcessor 1056\n",
      "NumericalizeProcessor 1056\n",
      "OpenFileProcessor 1056\n",
      "LMLabelList 1056\n",
      "LMTextList 1056\n",
      "SPProcessor 1056\n",
      "BaseTokenizer 1056\n",
      "SpacyTokenizer 1056\n",
      "Tokenizer 1056\n",
      "Vocab 1056\n",
      "fix_html 136\n",
      "replace_all_caps 136\n",
      "replace_rep 136\n",
      "replace_wrep 136\n",
      "rm_useless_spaces 136\n",
      "spec_add_spaces 136\n",
      "BOS 54\n",
      "EOS 54\n",
      "FLD 54\n",
      "UNK 54\n",
      "PAD 54\n",
      "TK_MAJ 54\n",
      "TK_UP 53\n",
      "TK_REP 54\n",
      "TK_WREP 55\n",
      "deal_caps 136\n",
      "EmbeddingDropout 1464\n",
      "LinearDecoder 1056\n",
      "AWD_LSTM 1464\n",
      "RNNDropout 1464\n",
      "SequentialRNN 1184\n",
      "WeightDropout 1464\n",
      "dropout_mask 136\n",
      "awd_lstm_lm_split 136\n",
      "awd_lstm_clas_split 136\n",
      "awd_lstm_lm_config 648\n",
      "awd_lstm_clas_config 648\n",
      "Activation 1056\n",
      "PositionalEncoding 1056\n",
      "GeLU 1184\n",
      "Swish 1184\n",
      "feed_forward 136\n",
      "MultiHeadAttention 1056\n",
      "MultiHeadRelativeAttention 1056\n",
      "DecoderLayer 1056\n",
      "Transformer 1056\n",
      "TransformerXL 1056\n",
      "tfmer_lm_config 648\n",
      "tfmer_clas_config 648\n",
      "tfmer_lm_split 136\n",
      "tfmer_clas_split 136\n",
      "tfmerXL_lm_config 648\n",
      "tfmerXL_clas_config 648\n",
      "tfmerXL_lm_split 136\n",
      "tfmerXL_clas_split 136\n",
      "TextClassificationInterpretation 1056\n",
      "text 80\n",
      "_i2 63\n",
      "bs 28\n",
      "_i3 77\n",
      "path 112\n",
      "_i4 97\n",
      "_i5 121\n",
      "data_clas 56\n",
      "_i6 187\n",
      "_i7 68\n",
      "_7 56\n",
      "_i8 138\n",
      "_i9 68\n",
      "lr 24\n",
      "_i10 138\n",
      "_i11 68\n",
      "_i12 137\n",
      "_i13 69\n",
      "_i14 60\n",
      "_i15 60\n",
      "_i16 53\n",
      "_i17 55\n",
      "_i18 54\n",
      "_i19 212\n",
      "print_function 56\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function  # for Python2\n",
    "import sys\n",
    "\n",
    "local_vars = list(locals().items())\n",
    "for var, obj in local_vars:\n",
    "    print(var, sys.getsizeof(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5fa9055f39ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
     ]
    }
   ],
   "source": [
    "del learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
